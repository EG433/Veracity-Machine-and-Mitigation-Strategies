{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e5b5d73c-9c59-46b1-a99e-99d6872f0e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mesop as me\n",
    "import time\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "import google.generativeai as genai\n",
    "import chromadb\n",
    "import PyPDF2  # For extracting text from PDF files\n",
    "import csv\n",
    "import pandas as pd\n",
    "from google.generativeai.types import content_types\n",
    "from collections.abc import Iterable\n",
    "from utils import *\n",
    "import asyncio\n",
    "from prediction_engine import PredictionEngine\n",
    "# import google_custom_search\n",
    "import serpapi\n",
    "# from dotenv import load_dotenv\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Remember to set your API key here\n",
    "os.environ['GOOGLE_API_KEY'] = 'AIzaSyCfPvWk29ewfisNYnIVMncN5b93QV34xbI'\n",
    "\n",
    "# Initialize API\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "# Initialize Chroma client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(\"my_collection\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "# Instantiate the GenAI model\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "obj_funcs = [repetition_analysis, origin_tracing, evidence_verification, omission_checks, exaggeration_analysis, target_audience_assessment]\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro-002\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "# fcmodel = genai.GenerativeModel(\n",
    "#     model_name=\"gemini-1.5-pro-002\",\n",
    "#     generation_config=generation_config,\n",
    "#     tools= obj_funcs,\n",
    "# )\n",
    "\n",
    "def tool_config_from_mode(mode: str, fns: Iterable[str] = ()):\n",
    "    \"\"\"Create a tool config with the specified function calling mode.\"\"\"\n",
    "    return content_types.to_tool_config(\n",
    "        {\"function_calling_config\": {\"mode\": mode, \"allowed_function_names\": fns}}\n",
    "    )\n",
    "\n",
    "chat_session = model.start_chat(history=[])\n",
    "tool_config = tool_config_from_mode(\"auto\")\n",
    "\n",
    "# google = google_custom_search.CustomSearch(apikey=\"your api_key\", engine_id=\"your engine_id\")\n",
    "\n",
    "@me.stateclass\n",
    "class State:\n",
    "    input: str = \"\"\n",
    "    output: str = \"\"\n",
    "    in_progress: bool = False\n",
    "    db_input: str = \"\"\n",
    "    db_output: str = \"\"\n",
    "    file: me.UploadedFile | None = None\n",
    "    pdf_text: str = \"\"\n",
    "    is_training: bool = False\n",
    "    training_status: str = \"\"\n",
    "    training_error: str = \"\"\n",
    "\n",
    "_prediction_engine = None\n",
    "def get_prediction_engine():\n",
    "    global _prediction_engine\n",
    "    if _prediction_engine is None:\n",
    "        _prediction_engine = PredictionEngine()\n",
    "    return _prediction_engine\n",
    "\n",
    "# Master function for the page\n",
    "@me.page(path=\"/\")\n",
    "def page():\n",
    "    with me.box(\n",
    "        style=me.Style(\n",
    "            background=\"#fff\",\n",
    "            min_height=\"calc(100% - 48px)\",\n",
    "            padding=me.Padding(bottom=16),\n",
    "        )\n",
    "    ):\n",
    "        with me.box(\n",
    "            style=me.Style(\n",
    "                width=\"min(720px, 100%)\",\n",
    "                margin=me.Margin.symmetric(horizontal=\"auto\"),\n",
    "                padding=me.Padding.symmetric(horizontal=16),\n",
    "            )\n",
    "        ):\n",
    "            header_text()\n",
    "            train_predictive_section()\n",
    "            uploader()\n",
    "            display_pdf_text()\n",
    "            chat_input()\n",
    "            output()\n",
    "            db_input()\n",
    "            db_output()\n",
    "            # convert_store_lp_data()\n",
    "            # convert_store_predai_data()\n",
    "        footer()\n",
    "\n",
    "def header_text():\n",
    "    with me.box(\n",
    "        style=me.Style(\n",
    "            padding=me.Padding(top=64, bottom=36),\n",
    "        )\n",
    "    ):\n",
    "        me.text(\n",
    "            \"Veracity Machine\",\n",
    "            style=me.Style(\n",
    "                font_size=36,\n",
    "                font_weight=700,\n",
    "                background=\"linear-gradient(90deg, #4285F4, #AA5CDB, #DB4437) text\",\n",
    "                color=\"transparent\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "def train_predictive(event: me.ClickEvent):\n",
    "    state = me.state(State)\n",
    "    if state.is_training:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        state.is_training = True\n",
    "        state.training_status = \"Initializing prediction engine...\"\n",
    "        yield\n",
    "        \n",
    "        # Initialize prediction engine\n",
    "        engine = get_prediction_engine()\n",
    "        \n",
    "        state.training_status = \"Loading dataset and preparing models...\"\n",
    "        yield\n",
    "        \n",
    "        # Start the training process\n",
    "        engine.load_dataset_and_prepare_models()\n",
    "        \n",
    "        state.training_status = \"Training complete!\"\n",
    "        yield\n",
    "        \n",
    "    except Exception as e:\n",
    "        state.training_error = f\"Training failed: {str(e)}\"\n",
    "    finally:\n",
    "        state.is_training = False\n",
    "        yield\n",
    "\n",
    "# Button element for training predictive model\n",
    "def train_predictive_section():\n",
    "    state = me.state(State)\n",
    "    \n",
    "    # Container box\n",
    "    with me.box(\n",
    "        style=me.Style(\n",
    "            padding=me.Padding.all(16),\n",
    "            margin=me.Margin(top=36),\n",
    "        )\n",
    "    ):\n",
    "        # Training button without context manager\n",
    "        me.button(\n",
    "            \"Train Classifier\" if not state.is_training else \"Training in Progress...\",\n",
    "            on_click=train_predictive,\n",
    "            disabled=state.is_training,\n",
    "            style=me.Style(width=\"100%\")\n",
    "        )\n",
    "        \n",
    "        # Show spinner if training\n",
    "        if state.is_training:\n",
    "            with me.box(\n",
    "                style=me.Style(\n",
    "                    margin=me.Margin(top=8),\n",
    "                    display=\"flex\",\n",
    "                    justify_content=\"center\"\n",
    "                )\n",
    "            ):\n",
    "                me.progress_spinner()\n",
    "        \n",
    "        # Status message\n",
    "        if state.training_status:\n",
    "            with me.box(\n",
    "                style=me.Style(\n",
    "                    padding=me.Padding.all(12),\n",
    "                    margin=me.Margin(top=8),\n",
    "                    background=\"#F0F4F9\",\n",
    "                    border_radius=8\n",
    "                )\n",
    "            ):\n",
    "                me.text(state.training_status)\n",
    "        \n",
    "        # Error message\n",
    "        if state.training_error:\n",
    "            with me.box(\n",
    "                style=me.Style(\n",
    "                    padding=me.Padding.all(12),\n",
    "                    margin=me.Margin(top=8),\n",
    "                    background=\"#FEE2E2\",\n",
    "                    border_radius=8\n",
    "                )\n",
    "            ):\n",
    "                me.text(\n",
    "                    state.training_error,\n",
    "                    style=me.Style(color=\"#DC2626\")\n",
    "                )\n",
    "\n",
    "# Upload function for PDF article\n",
    "def uploader():\n",
    "    state = me.state(State)\n",
    "    with me.box(style=me.Style(padding=me.Padding.all(15))):\n",
    "        me.uploader(\n",
    "            label=\"Upload PDF\",\n",
    "            accepted_file_types=[\"application/pdf\"],\n",
    "            on_upload=handle_upload,\n",
    "            type=\"flat\",\n",
    "            color=\"primary\",\n",
    "            style=me.Style(font_weight=\"bold\"),\n",
    "        )\n",
    "\n",
    "        if state.file and state.file.mime_type == 'application/pdf':\n",
    "            with me.box(style=me.Style(margin=me.Margin.all(10))):\n",
    "                me.text(f\"File name: {state.file.name}\")\n",
    "                me.text(f\"File size: {state.file.size} bytes\")\n",
    "                me.text(f\"File type: {state.file.mime_type}\")\n",
    "                # Extract text from the PDF after upload\n",
    "                extract_text_from_pdf(state.file)\n",
    "\n",
    "def handle_upload(event: me.UploadEvent):\n",
    "    state = me.state(State)\n",
    "    state.file = event.file\n",
    "\n",
    "def extract_text_from_pdf(file: me.UploadedFile):\n",
    "    \"\"\"Extracts text from the uploaded PDF file and stores it in the state.\"\"\"\n",
    "    state = me.state(State)\n",
    "    \n",
    "    # Wrap the bytes content in a BytesIO object\n",
    "    pdf_file = io.BytesIO(file.getvalue())\n",
    "\n",
    "    # Initialize the PDF reader\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)  \n",
    "    extracted_text = \"\"\n",
    "    \n",
    "    # Extract text from each page\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        extracted_text += page.extract_text()\n",
    "\n",
    "    state.pdf_text = extracted_text  # Store extracted PDF text in state\n",
    "\n",
    "def display_pdf_text():\n",
    "    \"\"\"Display the extracted PDF text.\"\"\"\n",
    "    state = me.state(State)\n",
    "    if state.pdf_text:\n",
    "        with me.box(style=me.Style(padding=me.Padding.all(10))):\n",
    "            me.text(\"Extracted PDF Content:\")\n",
    "            me.text(state.pdf_text)  # Display the entire text\n",
    "\n",
    "# User input for GenAI prompting\n",
    "def chat_input():\n",
    "    state = me.state(State)\n",
    "    with me.box(\n",
    "        style=me.Style(\n",
    "            padding=me.Padding.all(8),\n",
    "            background=\"white\",\n",
    "            display=\"flex\",\n",
    "            width=\"100%\",\n",
    "            border=me.Border.all(\n",
    "                me.BorderSide(width=0, style=\"solid\", color=\"black\")\n",
    "            ),\n",
    "            border_radius=12,\n",
    "            box_shadow=\"0 10px 20px #0000000a, 0 2px 6px #0000000a, 0 0 1px #0000000a\",\n",
    "        )\n",
    "    ):\n",
    "        with me.box(\n",
    "            style=me.Style(\n",
    "                flex_grow=1,\n",
    "            )\n",
    "        ):\n",
    "            me.native_textarea(\n",
    "                value=state.input,\n",
    "                autosize=True,\n",
    "                min_rows=4,\n",
    "                placeholder=\"Enter your customized prompt, or will do default FCOT if empty.\",\n",
    "                style=me.Style(\n",
    "                    padding=me.Padding(top=16, left=16),\n",
    "                    background=\"white\",\n",
    "                    outline=\"none\",\n",
    "                    width=\"100%\",\n",
    "                    overflow_y=\"auto\",\n",
    "                    border=me.Border.all(\n",
    "                        me.BorderSide(style=\"none\"),\n",
    "                    ),\n",
    "                ),\n",
    "                on_blur=textarea_on_blur,\n",
    "            )\n",
    "        with me.content_button(type=\"icon\", on_click=click_send):\n",
    "            me.icon(\"send\")\n",
    "\n",
    "# def chat_window():\n",
    "#     state = me.state(State)\n",
    "#     with me.box(\n",
    "#         style=me.Style(\n",
    "#             padding=me.Padding.all(8),\n",
    "#             background=\"white\",\n",
    "#             display=\"flex\",\n",
    "#             width=\"100%\",\n",
    "#             border=me.Border.all(\n",
    "#                 me.BorderSide(width=0, style=\"solid\", color=\"black\")\n",
    "#             ),\n",
    "#             border_radius=12,\n",
    "#             box_shadow=\"0 10px 20px #0000000a, 0 2px 6px #0000000a, 0 0 1px #0000000a\",\n",
    "#         )\n",
    "#     ):\n",
    "#         # Input area\n",
    "#         with me.box(style=me.Style(flex_grow=1)):\n",
    "#             me.native_textarea(\n",
    "#                 value=state.input,\n",
    "#                 autosize=True,\n",
    "#                 min_rows=4,\n",
    "#                 placeholder=\"Enter your prompt or query. Search results will be automatically included.\",\n",
    "#                 style=me.Style(\n",
    "#                     padding=me.Padding(top=16, left=16),\n",
    "#                     background=\"white\",\n",
    "#                     outline=\"none\",\n",
    "#                     width=\"100%\",\n",
    "#                     overflow_y=\"auto\",\n",
    "#                     border=me.Border.all(\n",
    "#                         me.BorderSide(style=\"none\"),\n",
    "#                     ),\n",
    "#                 ),\n",
    "#                 on_blur=textarea_on_blur,\n",
    "#             )\n",
    "#         with me.content_button(type=\"icon\", on_click=click_chat_send):\n",
    "#             me.icon(\"send\")\n",
    "\n",
    "# def click_chat_send(e: me.ClickEvent):\n",
    "#     state = me.state(State)\n",
    "#     if not state.input.strip():\n",
    "#         return\n",
    "    \n",
    "#     state.in_progress = True\n",
    "#     user_prompt = state.input\n",
    "#     yield\n",
    "\n",
    "#     # Conduct a Google Custom Search query\n",
    "#     params = {\n",
    "#         \"q\": user_prompt,\n",
    "#         \"location\": \"San Diego, California, United States\",\n",
    "#         \"hl\": \"en\",\n",
    "#         \"gl\": \"us\",\n",
    "#         \"google_domain\": \"google.com\",\n",
    "#         \"api_key\": \"114093c99f6f7f86c455b6e302123c175cf5aefcbd9affa7352f27353dc13d61\"\n",
    "#     }\n",
    "\n",
    "#     search = GoogleSearch(params)\n",
    "#     results = search.get_dict()\n",
    "\n",
    "#     # Create a combined prompt with search results\n",
    "#     google_search = f\"Search Results:\\n\" + \"\\n\".join(\n",
    "#         [f\"{i+1}. {result['title']}: {result['snippet']} ({result['url']})\"\n",
    "#          for i, result in enumerate(results)]\n",
    "#     )\n",
    "\n",
    "#     return google_search\n",
    "\n",
    "# def search_google_custom(query: str):\n",
    "#     results = google.search(query)\n",
    "#     search_data = []\n",
    "#     for result in results:\n",
    "#         search_data.append({\n",
    "#             \"title\": result.title,\n",
    "#             \"url\": result.url,\n",
    "#             \"snippet\": result.snippet\n",
    "#         })\n",
    "#     return search_data\n",
    "\n",
    "def textarea_on_blur(e: me.InputBlurEvent):\n",
    "    state = me.state(State)\n",
    "    state.input = e.value\n",
    "\n",
    "def click_send(e: me.ClickEvent):\n",
    "    state = me.state(State)\n",
    "    if not state.input.strip():  # Check if input is empty or contains only whitespace\n",
    "        state.input = \"Default input text here.\"  # Default FCT prompt if no input is provided\n",
    "        return\n",
    "    \n",
    "    state.in_progress = True\n",
    "    input_text = state.input\n",
    "\n",
    "    '''TODO: Potential implementation of chunking here (By statement)'''\n",
    "\n",
    "    engine = get_prediction_engine()\n",
    "    predict_score = engine.predict_new_example(convert_statement_to_series(state.pdf_text))['overall']\n",
    "\n",
    "    # top_100_statements = get_top_100_statements(input_text)\n",
    "    fct_prompt = generate_fct_prompt(state.pdf_text, predict_score)\n",
    "    combined_input = combine_pdf_and_prompt(fct_prompt, state.pdf_text)  # Combine prompt with PDF text\n",
    "    top_100_statements = get_top_100_statements(combined_input)\n",
    "\n",
    "    for chunk in call_api(combined_input):\n",
    "        state.output += chunk\n",
    "        yield\n",
    "\n",
    "    state.output += '\\n\\n The probability of the statement truthness:\\n\\n' + str(top_100_statements) + '\\n\\n'\n",
    "\n",
    "    state.in_progress = False\n",
    "    yield\n",
    "\n",
    "def convert_statement_to_series(statement):\n",
    "    '''\n",
    "    TODO: Will need to extract speaker info later on\n",
    "    '''\n",
    "    if not isinstance(statement, str):\n",
    "        return pd.Series(['','', '', '', '', '', '','','0.0','0.0','0.0','0.0','0.0', '', ''])\n",
    "    subject = ''\n",
    "    speaker = ''\n",
    "    speaker_title = ''\n",
    "    state = ''\n",
    "    party_aff = ''\n",
    "    context = ''\n",
    "    return pd.Series(['','',subject, statement, speaker, speaker_title, state, party_aff,'0.0','0.0','0.0','0.0','0.0', context, ''])\n",
    "\n",
    "# Fractal COT & Function Call\n",
    "# Define the complex objective functions\n",
    "frequency_heuristic = [\n",
    "    {\"description\": \"Micro Factor 1: Repetition Analysis\", \"details\": \"Analyzing wider coverage helps assess consensus. If multiple independent sources confirm manipulation, it strengthens the claim. Even widespread agreement about deceptive editing wouldn't automatically justify government action against CBS. The First Amendment protects against content-based restrictions.\"},\n",
    "    {\"description\": \"Micro Factor 2: Origin Tracing\", \"details\": \"Confirmed sources are critical. Discrepancies between reporting and original sources raise red flags.\"},\n",
    "    {\"description\": \"Micro Factor 3: Evidence Verification\", \"details\": \"Expert analysis is the most crucial element. Expert opinions on video manipulation are essential. Expert testimony would be necessary for any legal action alleging manipulation, though such a case would face significant First Amendment hurdles.\"}\n",
    "]\n",
    "\n",
    "misleading_intentions = [\n",
    "    {\"description\": \"Micro Factor 1: Omission Checks\", \"details\": \"Assess the omissions' impact. Did they distort the message? Did they create a demonstrably false representation? (Proving this is difficult).\"},\n",
    "    {\"description\": \"Micro Factor 2: Exaggeration Analysis\", \"details\": \"Evaluate the 'scandal' claim. Does the evidence support it, or is it hyperbole? Does the situation, even if accurately reported, justify calls for license revocation under existing legal and constitutional frameworks?\"},\n",
    "    {\"description\": \"Micro Factor 3: Target Audience Assessment\", \"details\": \"Analyze audience manipulation. Identify targeting tactics (language, framing). While such tactics can be ethically questionable, they are generally protected speech unless they involve provable falsehoods and meet the very high legal bar for defamation or incitement.\"}\n",
    "]\n",
    "\n",
    "def generate_fct_prompt(input_text, predict_score, iterations=3):\n",
    "    prompt = f'Use {iterations} iterations to check the veracity score of this news article. In each, determine what you missed in the previous iteration based on your evaluation of the objective functions. Also put the result from RAG into consideration/rerank.'\n",
    "    prompt += f'\\n\\n RAG:\\n Here, out of six potential labels (true, mostly-true, half-true, barely-true, false, pants-fire), this is the truthfulness label predicted using a classifier model: {predict_score}.\\n These are the top 100 related statement in LiarPLUS dataset that related to this news article: {get_top_100_statements(input_text)}'\n",
    "    for i in range(1, iterations + 1):\n",
    "        prompt += f\"Iteration {i}: Evaluate the text based on the following objectives and also on microfactors, give explanations on each of these:\\n\"\n",
    "        prompt += \"\\nFactuality Factor 1: Frequency Heuristic:\\n\"\n",
    "        for fh in frequency_heuristic:\n",
    "            prompt += f\"{fh['description']}: {fh['details']}\\n\"\n",
    "        prompt += \"\\nFactuality Factor 2: Misleading Intentions:\\n\"\n",
    "        for mi in misleading_intentions:\n",
    "            prompt += f\"{mi['description']}: {mi['details']}\\n\"\n",
    "        prompt += \"\\nProvide a percentage score and explanation for each objective function ranking and its microfactors.\\n\\n\"\n",
    "    prompt += \"Final Evaluation: Return an exact numeric veracity score for the text, and provide a matching label out of these six [true, mostly-true, half-true, barely-true, false, pants-fire]\"\n",
    "    return prompt\n",
    "\n",
    "def combine_pdf_and_prompt(prompt: str, pdf_text: str) -> str:\n",
    "    \"\"\"Combines the user's prompt with the extracted PDF text.\"\"\"\n",
    "    if not pdf_text:\n",
    "        return prompt\n",
    "    return f\"Prompt: {prompt}\\n\\nPDF Content: {pdf_text}\"  # Combine entire text\n",
    "\n",
    "def chunk_pdf_text(pdf_text: str) -> list[str]:\n",
    "    if not pdf_text:\n",
    "        return []\n",
    "        \n",
    "    prompt = \"\"\"Split the following text into logical chunks (max 10 chunks) of reasonable size. \n",
    "    Preserve complete paragraphs and maintain context. Return ONLY the chunks as a numbered list, with no additional text.\n",
    "    Format each chunk like:\n",
    "    1. [chunk text]\n",
    "    2. [chunk text]\n",
    "    etc.\n",
    "\n",
    "    Text to split:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    response = chat_session.send_message(prompt.format(text=pdf_text))\n",
    "    chunks_text = response.text.strip()\n",
    "    \n",
    "    # Split on numbered lines and clean up\n",
    "    chunks = []\n",
    "    for line in chunks_text.split('\\n'):\n",
    "        # Skip empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        # Remove the number prefix and clean whitespace\n",
    "        chunk = line.split('.', 1)[-1].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "    return chunks\n",
    "   \n",
    "    \n",
    "# Sends API call to GenAI model with user input\n",
    "def call_api(input_text):\n",
    "    context = \" \"\n",
    "    # Add context to the prompt\n",
    "    full_prompt = f\"Context: {context}\\n\\nUser: {input_text}\"\n",
    "    response = chat_session.send_message(full_prompt)\n",
    "    # time.sleep(0.5)\n",
    "    # yield response.candidates[0].content.parts[0].text\n",
    "    yield response.parts[0].text\n",
    "\n",
    "# Display output from GenAI model\n",
    "def output():\n",
    "    state = me.state(State)\n",
    "    if state.output or state.in_progress:\n",
    "        with me.box(\n",
    "            style=me.Style(\n",
    "                background=\"#F0F4F9\",\n",
    "                padding=me.Padding.all(16),\n",
    "                border_radius=16,\n",
    "                margin=me.Margin(top=36),\n",
    "            )\n",
    "        ):\n",
    "            if state.output:\n",
    "                me.markdown(state.output)\n",
    "            if state.in_progress:\n",
    "                with me.box(style=me.Style(margin=me.Margin(top=16))):\n",
    "                    me.progress_spinner()\n",
    "\n",
    "# Manual add function for database (would be deprecated when dataset processing is fully automized)\n",
    "def db_input():\n",
    "    state = me.state(State)\n",
    "    with me.box(\n",
    "        style=me.Style(\n",
    "            padding=me.Padding.all(8),\n",
    "            background=\"white\",\n",
    "            display=\"flex\",\n",
    "            width=\"100%\",\n",
    "            border=me.Border.all(\n",
    "                me.BorderSide(width=0, style=\"solid\", color=\"black\")\n",
    "            ),\n",
    "            border_radius=12,\n",
    "            box_shadow=\"0 10px 20px #0000000a, 0 2px 6px #0000000a, 0 0 1px #0000000a\",\n",
    "            margin=me.Margin(top=36),\n",
    "        )\n",
    "    ):\n",
    "        with me.box(\n",
    "            style=me.Style(\n",
    "                flex_grow=1,\n",
    "            )\n",
    "        ):\n",
    "            me.native_textarea(\n",
    "                value=state.db_input,\n",
    "                autosize=True,\n",
    "                min_rows=4,\n",
    "                placeholder=\"Enter statement to be added to database\",\n",
    "                style=me.Style(\n",
    "                    padding=me.Padding(top=16, left=16),\n",
    "                    background=\"white\",\n",
    "                    outline=\"none\",\n",
    "                    width=\"100%\",\n",
    "                    overflow_y=\"auto\",\n",
    "                    border=me.Border.all(\n",
    "                        me.BorderSide(style=\"none\"),\n",
    "                    ),\n",
    "                ),\n",
    "                on_blur=db_textarea_on_blur,\n",
    "            )\n",
    "        with me.content_button(type=\"icon\", on_click=click_add_to_db):\n",
    "            me.icon(\"add\")\n",
    "\n",
    "def db_textarea_on_blur(e: me.InputBlurEvent):\n",
    "    state = me.state(State)\n",
    "    state.db_input = e.value\n",
    "\n",
    "def click_add_to_db(e: me.ClickEvent):\n",
    "    state = me.state(State)\n",
    "    if not state.db_input:\n",
    "        return\n",
    "\n",
    "    collection.add(\n",
    "                documents=[state.db_input],         \n",
    "                metadatas=[{\"source\": 'manual', \"row_index\": 'none'}],            \n",
    "                ids=[f\"doc_{int(time.time())}\"]   \n",
    "            )\n",
    "    \n",
    "    state.db_output = f\"Manually added to database: {state.db_input[:50]}...\"\n",
    "    state.db_input = \"\"\n",
    "    yield\n",
    "\n",
    "# Display database state for added vector\n",
    "def db_output():\n",
    "    state = me.state(State)\n",
    "    if state.db_output:\n",
    "        with me.box(\n",
    "            style=me.Style(\n",
    "                background=\"#F0F4F9\",\n",
    "                padding=me.Padding.all(16),\n",
    "                border_radius=16,\n",
    "                margin=me.Margin(top=36),\n",
    "            )\n",
    "        ):\n",
    "            me.text(state.db_output)\n",
    "\n",
    "# Page footer\n",
    "def footer():\n",
    "    with me.box(\n",
    "        style=me.Style(\n",
    "            position=\"sticky\",\n",
    "            bottom=0,\n",
    "            padding=me.Padding.symmetric(vertical=16, horizontal=16),\n",
    "            width=\"100%\",\n",
    "            background=\"#F0F4F9\",\n",
    "            font_size=14,\n",
    "        )\n",
    "    ):\n",
    "        me.html(\n",
    "            \"Made with <a href='https://google.github.io/mesop/'>Mesop</a>\",\n",
    "        )\n",
    "\n",
    "def get_top_100_statements(user_input):\n",
    "    # Query ChromaDB for top 100 similar inputs based on cosine similarity\n",
    "    results = collection.query(\n",
    "        query_texts=[user_input],\n",
    "        n_results=100,\n",
    "        include=[\"documents\"],\n",
    "        where = {\"source\": {\"$in\": [\"train\", \"test\", \"validate\"]}}\n",
    "    )\n",
    "\n",
    "    #store and return the number of each 100 statements in dictionary\n",
    "    statement_dic = {}\n",
    "    for i in range(100):\n",
    "        statement = results['documents'][0][i].split(', ')[2]\n",
    "        if statement not in statement_dic:\n",
    "            statement_dic[statement] = 1\n",
    "        else:\n",
    "            statement_dic[statement] += 1   \n",
    "    return statement_dic\n",
    "\n",
    "\n",
    "#converting lierplus dataset and store in datebase\n",
    "def convert_store_lp_data():\n",
    "    train_data = pd.read_csv('data/train2.tsv', sep='\\t',header=None, dtype=str)\n",
    "    test_data = pd.read_csv('data/test2.tsv', sep='\\t', header=None, dtype=str)\n",
    "    validate_data = pd.read_csv('data/val2.tsv', sep='\\t',header=None, dtype=str)\n",
    "\n",
    "    datasets = [\n",
    "        {\"data\": train_data, \"source\": \"train\"},\n",
    "        {\"data\": test_data, \"source\": \"test\"},\n",
    "        {\"data\": validate_data, \"source\": \"validate\"}\n",
    "    ]\n",
    "   \n",
    "    for dataset in datasets:\n",
    "        source = dataset[\"source\"]\n",
    "        data = dataset[\"data\"]\n",
    "     # Iterate over each row, combining it into a paragraph and processing it\n",
    "        for idx, row in data.iterrows():\n",
    "            # Combine row data into a single string (statement + metadata)\n",
    "            statement = ', '.join(row.astype(str))\n",
    "\n",
    "            # Store statement and metadata in ChromaDB\n",
    "            collection.add(\n",
    "                documents=[statement],         \n",
    "                metadatas=[{\"source\": source, \"row_index\": idx}],            \n",
    "                ids=[f\"{source}_doc_{idx}\"]   \n",
    "            )\n",
    "\n",
    "    print(\"All data has been successfully processed and stored in ChromaDB.\")\n",
    "\n",
    "\n",
    "#converting predictive ai generated dataset and store in datebase\n",
    "def convert_store_predai_data():\n",
    "    train_data = pd.read_csv('PredictiveAI/train_data_full.tsv', sep='\\t',header=None, dtype=str)\n",
    "    test_data = pd.read_csv('PredictiveAI/test_data_full.tsv', sep='\\t', header=None, dtype=str)\n",
    "    validate_data = pd.read_csv('PredictiveAI/val_data_full.tsv', sep='\\t',header=None, dtype=str)\n",
    "    micro_factors = pd.read_csv('PredictiveAI/average_scores.tsv', sep='\\t',header=None, dtype=str)\n",
    "\n",
    "    datasets = [\n",
    "        {\"data\": train_data, \"source\": \"train\"},\n",
    "        {\"data\": test_data, \"source\": \"test\"},\n",
    "        {\"data\": validate_data, \"source\": \"validate\"},\n",
    "        {\"data\": micro_factors, \"source\": \"factors\"}\n",
    "    ]\n",
    "   \n",
    "    for dataset in datasets:\n",
    "        source = dataset[\"source\"]\n",
    "        data = dataset[\"data\"]\n",
    "     # Iterate over each row, combining it into a paragraph and processing it\n",
    "        for idx, row in data.iterrows():\n",
    "            # Combine row data into a single string (statement + metadata)\n",
    "            statement = ', '.join(row.astype(str))\n",
    "\n",
    "            # Store statement and metadata in ChromaDB\n",
    "            collection.add(\n",
    "                documents=[statement],         \n",
    "                metadatas=[{\"source\": source, \"row_index\": idx}],            \n",
    "                ids=[f\"{source}_doc_{idx}\"]   \n",
    "            )\n",
    "\n",
    "    print(\"All PredAI data has been successfully processed and stored in ChromaDB.\")\n",
    "\n",
    "def obtain_model_accuracy(test_size=20):\n",
    "    \"\"\"Doesn't work when function calling is activated, test_size=20 is when Gemini actually gives useful responses\"\"\"\n",
    "    # Load test statements\n",
    "    test = pd.read_csv(\"../data/test2.tsv\", sep=\"\\t\", header=None, dtype=str).drop(columns=[0]).sample(n=test_size, random_state=15)\n",
    "    test_statements = [datapoint[1][3] for datapoint in test.iterrows()]\n",
    "\n",
    "    print(test.iloc[:,1].to_list())\n",
    "\n",
    "    predictions = []\n",
    "    rag_retrievals = []\n",
    "    engine = get_prediction_engine()\n",
    "    engine.load_dataset_and_prepare_models()\n",
    "    for statement in test.iterrows():\n",
    "        predict_score = engine.predict_new_example(statement[1])['overall'][0]\n",
    "        predictions.append(predict_score)\n",
    "        rag_100 = get_top_100_statements(statement[1][3])\n",
    "        rag_retrievals.append(rag_100)\n",
    "\n",
    "    prompt = 'For each of these news statements, use 3 iterations to determine the veracity within the statement. In each iteration, determine what you missed in the previous iteration based on your evaluation of the objective functions. Also put the result from RAG into consideration/rerank.'\n",
    "    for i in range(1, 4):\n",
    "        prompt += f\"Iteration {i}: Evaluate the text based on the following objectives and also on microfactors:\\n\"\n",
    "        prompt += \"\\nFactuality Factor 1: Frequency Heuristic:\\n\"\n",
    "        for fh in frequency_heuristic:\n",
    "            prompt += f\"{fh['description']}: {fh['details']}\\n\"\n",
    "        prompt += \"\\nFactuality Factor 2: Misleading Intentions:\\n\"\n",
    "        for mi in misleading_intentions:\n",
    "            prompt += f\"{mi['description']}: {mi['details']}\\n\"\n",
    "        prompt += \"\\nDo not provide any explanation and only give the final output.\\n\\n\"\n",
    "    prompt += \"Final output: For each of the statements, return an exact numeric veracity score for the text, and provide a matching label out of these six [true, mostly-true, half-true, barely-true, false, pants-fire]. Return the labels within <> markers.\\n\"\n",
    "    prompt += f\"Rule: you have to return the exact number of outputs({test_size}) as the number of inputs. Make sure this rule is followed by adding an index to each output.\"\n",
    "    prompt += f'\\n\\n RAG:\\n Here, out of six potential labels (true, mostly-true, half-true, barely-true, false, pants-fire), these are the respective truthfulness labels predicted using a classifier model: None. \\n RAG: These are the top 100 related statements in LiarPLUS dataset that related to each of the corresponding news statement: {rag_retrievals}'\n",
    "\n",
    "\n",
    "    combined_input = f\"Prompt: {prompt}\\n\\n Content: {test_statements}\"\n",
    "    \n",
    "    output = ''\n",
    "    for chunk in call_api(combined_input):\n",
    "        output += chunk\n",
    "\n",
    "    print(output)\n",
    "    # Extract label from response\n",
    "    try:\n",
    "        labels = [l.lower() for l in re.findall(r'<([\\w-]+)>', output)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting label: {str(e)}\")\n",
    "\n",
    "    print(labels)\n",
    "    test_accuracy = accuracy_score(test.iloc[:,1].to_list(), labels)\n",
    "    report = classification_report(test.iloc[:,1].to_list(), labels)\n",
    "\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print('=====================\\n\\n')\n",
    "    print(report)\n",
    "\n",
    "# # Verify the data count in ChromaDB\n",
    "# doc_count = collection.count()\n",
    "# print(f\"Total documents stored in ChromaDB: {doc_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0795805d-b6e2-4bb9-9611-6baef1ee37b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pants-fire', 'false', 'false', 'mostly-true', 'pants-fire', 'mostly-true', 'false', 'true', 'half-true', 'true', 'false', 'half-true', 'half-true', 'half-true', 'true', 'half-true', 'mostly-true', 'true', 'barely-true', 'false']\n",
      "Loading dataset...\n",
      "Loaded pre-trained models successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 27.13it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 67.64it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 69.57it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 69.78it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 72.32it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 72.59it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 73.39it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 84.05it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 68.96it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 80.41it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 83.02it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 81.81it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 89.53it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 93.84it/s]\n",
      "Extracting embeddings: 100%|█████████████████████| 1/1 [00:00<00:00, 100.95it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 10.50it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 77.33it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 70.88it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 76.45it/s]\n",
      "Extracting embeddings: 100%|██████████████████████| 1/1 [00:00<00:00, 68.94it/s]\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Resource has been exhausted (e.g. check quota).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mobtain_model_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[80], line 731\u001b[0m, in \u001b[0;36mobtain_model_accuracy\u001b[0;34m(test_size)\u001b[0m\n\u001b[1;32m    728\u001b[0m combined_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_statements\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 731\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "Cell \u001b[0;32mIn[80], line 500\u001b[0m, in \u001b[0;36mcall_api\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Add context to the prompt\u001b[39;00m\n\u001b[1;32m    499\u001b[0m full_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 500\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# time.sleep(0.5)\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# yield response.candidates[0].content.parts[0].text\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m response\u001b[38;5;241m.\u001b[39mparts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/anaconda3/envs/DSC180/lib/python3.11/site-packages/google/generativeai/generative_models.py:578\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[0;34m(self, content, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid configuration: The chat functionality does not support `candidate_count` greater than 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     )\n\u001b[0;32m--> 578\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools_lib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_response(response\u001b[38;5;241m=\u001b[39mresponse, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_automatic_function_calling \u001b[38;5;129;01mand\u001b[39;00m tools_lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/DSC180/lib/python3.11/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/DSC180/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:830\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/DSC180/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DSC180/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DSC180/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/anaconda3/envs/DSC180/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/DSC180/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/anaconda3/envs/DSC180/lib/python3.11/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DSC180/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 Resource has been exhausted (e.g. check quota)."
     ]
    }
   ],
   "source": [
    "obtain_model_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ca5d1-8b38-4921-8818-a930a316e162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
